{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "from langchain import ConversationChain, LLMChain, OpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\n",
    "                               MessagesPlaceholder, PromptTemplate,\n",
    "                               SystemMessagePromptTemplate)\n",
    "from langchain.prompts.chat import (ChatPromptTemplate,\n",
    "                                    HumanMessagePromptTemplate,\n",
    "                                    SystemMessagePromptTemplate)\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs: Get predictions from a language model\n",
    "\n",
    "LangChain의 가장 기본적인 구성 요소는 입력값에 대해 LLM을 호출하는 것입니다. 이를 수행하는 간단한 예제를 살펴보겠습니다. 이를 위해, 회사가 무엇을 생산하는지에 기반하여 회사 이름을 생성하는 서비스를 구축한다고 가정해 봅시다. 이를 위해서는 먼저 LLM 래퍼를 가져와야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bright Footwear.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates: Manage prompts for LLMS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로, LLM을 사용할 때, 사용자 입력을 직접 보내는 것이 아니라, 사용자 입력을 받아 프롬프트(prompt)를 작성한 후 LLM에게 보냅니다. 이 문서에서는 LangChain을 사용하여 프롬프트 템플릿을 정의하는 방법을 설명합니다. 프롬프트 템플릿은 사용자 입력에서 필요한 정보만 추출하여 해당 정보를 이용해 프롬프트를 구성하는 것이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"colorful socks\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains: Combine LLMs and prompts in multi-step workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRainbow Toe Socks'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents: Dynamically Call Chains Based on User Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에이전트는 LLM(언어 모델)을 사용하여 어떤 작업을 수행하고 그 결과를 반환하는 도구입니다. 이 때, 도구는 구글 검색, 데이터베이스 조회, Python REPL, 다른 체인 등과 같이 특정한 작업을 수행하는 함수를 의미합니다. \n",
    "\n",
    "LLM은 에이전트의 핵심인 언어 모델을 의미합니다. \n",
    "\n",
    "에이전트는 지원하는 에이전트 클래스를 참조하는 문자열입니다. 이 노트북에서는 가장 간단하고 Highest Level API를 사용하여 기본적으로 사용하는 방법에 대해 설명합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoogleSearch.SERP_API_KEY = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
      "Action: Search\n",
      "Action Input: \"High temperature in SF yesterday\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHigh: 51.98ºf @12:56 AM Low: 50ºf @4:45 AM Approx. Precipitation / Rain Total: 0.063 in. 1hr. Precip / Rain Total (in.)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to use the calculator to raise the temperature to the .023 power.\n",
      "Action: Calculator\n",
      "Action Input: 51.98^.023\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.0951263641624882\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0951263641624882.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0951263641624882.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory: Add State to Chains and Agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 chain과 agent들은 모두 stateless하지만, 종종 이전 interaction에서 정보를 기억할 수 있도록 \"memory\"가 필요함\n",
    "\n",
    "\"short-term memory\"은 이전 메세지를 기억함으로 더 나은 대화를 할 수 있으며,\n",
    "\"long-term memory\"과 같이 시간이 지나더라도, 중요한 정보를 기억할 수 있습니다.\n",
    "\n",
    "LangChain은 이러한 목적으로 특별히 만들어진 여러 chain과을 제공합니다. 이 노트북에서는 위의 두가지 유형의 memory를 위해 ConversationChain을 사용하는 방법을 설명합니다.\n",
    "\n",
    "ConversationChain은 모든 이전 입력/출력을 기억하고, 전달되는 컨텍스트에 추가하는 간단한 유형의 메모리를 가지고 있습니다. 우리는 prompt를 볼 수 있도록 `verbose=True`로 설정하여 이 체인을 사용하는 방법을 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Language Model Application: Chat Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM 대신 Chat models을 사용할 수 있다는 것을 설명하고 있습니다. Chat models은 언어 모델의 변형입니다. Chat models은 언어 모델을 내부적으로 사용하지만, 제공하는 인터페이스는 \"텍스트 입력, 텍스트 출력\" API가 아니라 \"채팅 메시지\"가 입력과 출력으로 제공됩니다. Chat model API는 상대적으로 새로운 기술이기 때문에, 적절한 추상화 방법을 아직 찾고 있습니다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Message Completions from a Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Translate this sentence from English to French. I love programming.\"),\n",
    "]\n",
    "chat(messages)\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 73, 'completion_tokens': 16, 'total_tokens': 89}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to French. I love programming.\"),\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to French. I love artificial intelligence.\"),\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result\n",
    "# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 71, 'completion_tokens': 18, 'total_tokens': 89}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 73, 'completion_tokens': 16, 'total_tokens': 89}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output[\"token_usage\"]\n",
    "# -> {'prompt_tokens': 71, 'completion_tokens': 18, 'total_tokens': 89}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate`은 하나 이상의 `MessagePromptTemplate`으로부터 구성될 수 있으며, `format_prompt`를 사용하여 `PromptValue`를 반환할 수 있습니다. `PromptValue`는 llm 또는 chat 모델의 입력으로 사용할 수 있는 문자열 또는 Message 객체로 변환할 수 있습니다. 또한, 템플릿에서는 `from_template` 메서드를 사용하여 편리하게 `ChatPromptTemplate`을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(\n",
    "    chat_prompt.format_prompt(\n",
    "        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n",
    "    ).to_messages()\n",
    ")\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains with Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "# -> \"J'aime programmer.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents with Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3msinger Harry Styles\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to find Harry Styles' age and raise it to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Harry Styles age\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m29 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to calculate 29 raised to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"29^0.23\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.169459462491557\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: 2.169459462491557\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.169459462491557'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Add State to Chains and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! I am an AI language model designed to assist with various tasks such as answering questions, generating text, and providing recommendations. I was created using natural language processing techniques and machine learning algorithms, which allow me to understand and respond to human language. I am constantly learning and improving based on the interactions I have with users like you. Is there anything else you'd like to know?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")\n",
    "# -> 'Hello! How can I assist you today?'\n",
    "\n",
    "\n",
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
    "# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\n",
    "\n",
    "conversation.predict(input=\"Tell me about yourself.\")\n",
    "# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
